## Hadoop vs Spark


![Spark  Logo](https://www.clipartmax.com/png/full/78-780281_whats-new-in-apache-spark-apache-spark-logo.png)

Одна из особенностей самых выдающихся технических профессий — это открытая атмосфера, которая поощряет исследования и разработки. Тот же подход хорошо работает при сравнении таких продуктов, как Hadoop и Spark. Любой вариант предлагает качественную структуру с надёжными функциями. Итак, что же такое Spark и Hadoop? Какие варианты и подходы есть у этих двух компаний и есть ли области, в которых они предлагают идентичные услуги?

**Ключевой концепцией**, которую следует понимать при работе со Spark и Hadoop, является понятие больших данных. **Большие данные** — это сбор и изучение огромных объёмов информации с целью лучшего понимания рынка, клиентов и потребностей. Однако большие данные производят огромное количество информации. И для её обработки необходимо использовать **специальные методы распространения и анализа**. И Hadoop, и Spark делают это через распределённые среды компьютеров и приложений.

**Spark** — инфраструктура кластерных вычислений, сходная с Hadoop MapReduce. Однако Spark не занимается ни хранением файлов в файловой системе, ни управлением ресурсами. Spark обрабатывает данные **ещё быстрее с помощью встроенных коллекций RDD** (Resilient Distributed Datasets), которые дают возможность выполнять вычисления в больших кластерах. Благодаря RDD можно совершать такие операции, как *map, join, reduce*, записывать данные на диск и загружать их.

Рассмотрим таблицу сравнения Hadoop и Spark:

|Hadoop                |         Spark      |
|----------------------|--------------------|
|Обработка данных с использованием MapReduce  происходит медленнее| Обрабатывает данные в 100 раз быстрее|
|Выполняет пакетную обработку данных| Выполняет как пакетную обработку, так и обработку данных в реальном времени|
|Написан на Java, больше строк кода| В Spark меньше строк кода, так как он реализован на Scala|
---

Промежуточные данные вычислений не записываются на диск, а образуют своего рода **общую оперативную память**. Это позволяет разным рабочим процессам использовать общие переменные и их состояния.

### Отложенные вычисления

Spark приступает к выполнению запроса лишь при непосредственном обращении к нему (вывод на экран, запись конечных данных на диск). В этом случае срабатывает планировщик, соединяя все преобразования, написанные ранее.

Spark и Hadoop относятся к разным эпохам компьютерного дизайна и разработки. И это проявляется в том, как они обрабатывают данные. Hadoop должен управлять своими данными **в пакетном режиме** благодаря своей версии MapReduce. А это означает, что у него нет возможности **работать с данными в реальном времени по мере их поступления**. Это одновременно и преимущество, и недостаток — **пакетная обработка** — это эффективный метод работы с большими объёмами данных, но отсутствие метода обработки потоковых данных снижает производительность Hadoop.

Другое различие между Hadoop и Spark заключается в том, что Spark предоставляет **множество API**, которые можно использовать с несколькими источниками данных и языками. Кроме того, они более расширяемы, чем Hadoop API.

В заключение следует отметить, что различие между Hadoop и Spark заключается в том, что **Hadoop - это платформа Apache с открытым исходным кодом, которая позволяет распределенную обработку больших наборов данных по кластерам компьютеров с использованием простых моделей программирования**, тогда как **Spark - это среда кластерных вычислений, предназначенная для быстрых вычислений Hadoop**. Оба могут быть использованы для приложений, основанных на прогнозирующей аналитике, интеллектуальном анализе данных, машинном обучении и многом другом.

---
## Архитектура Spark

Приложения, которые создаются на базе фреймворка Spark, предназначены для работы в распределенной среде (например, в кластере, состоящем из нескольких узлов). **Архитектура распределенной (параллельной) среды включает в себя следующие компоненты**:

* драйвер Spark
* Spark-исполнители

![Avatar](https://sun9-west.userapi.com/sun9-63/s/v1/ig2/JWj3Z4hMohxWsB5m1qbcPvSID2j2CltG3mxapK_frMOlErRtoQtZLuI4kSSEXWAqNCwra13Mhvuq_vD6tNk6YfRL.jpg?size=625x704&quality=96&type=album)

### Драйвер  спарк
**Драйвер Spark – это процесс, который распределяет задачи, поступающие от пользователя по действующим исполнителям**. Таким образом, Spark-драйвер **преобразует пользовательское приложение на единицы исполнения**, которые называются задачи (tasks). **Экземпляр Spark-драйвера** запускается во время запуска сессии (драйвер Spark создает сессию) Spark при первом запуске приложения и остается активным до тех пор, эта сессия активна (приложение работает и сбой не произошел).
Приложение может разбиваться на несколько сотен и даже тысяч заданий (в зависимости от функционала). На основе составленного плана со всеми задачами **драйвер Spark контролирует передачу этих задач исполнителям**. При запуске каждый исполнитель регистрирует себя в драйвере.

### Спарк исполнители
**Spark-исполнители (Spark executors) – это рабочие процессы, которые отвечают за выполнение задач, приходящих из драйвера**. Исполнители запускаются только один раз при запуске приложения Spark и продолжают свою работу на протяжении всего жизненного цикла программы. **Они выполняют задачи, приходящие от драйвера и возвращают результат обратно драйверу Spark**. Каждому исполнителю, также, как и драйверу выделяется определенный объем оперативной памяти, значение которого по умолчанию составляет 1Гб. Каждый исполнитель имеет определенное число ядер (cores). Ядро исполнителя (executor core) отвечает за параллельное выполнение задач одним исполнителем

![Avatar](https://sun9-west.userapi.com/sun9-46/s/v1/ig2/RvEKNHggie9kmFVjLXdptCggFMxglitPzyCKwD-DY_beL5La_bQceXMxVhnEjzMxq46q-nZeJLyjXAeqwK1tSxhH.jpg?size=974x467&quality=96&type=album)

Чем больше количество ядер, тем больше задач может одновременно выполнять один исполнитель. Однако стоит контролировать количество ядер в каждом исполнителе, так как каждое ядро требует значительных затрат мощности процессора. Слишком большое количество ядер может привести к сбою приложения. 
Таким образом, распределенная **архитектура приложения Spark позволяет выполнять большие объемы Big Data задач, требующих высокое количество вычислений**. Все это делает framework Apache Spark весьма полезным средством для Data Scientist’а и разработчика Big Data приложений. 

---

### Исполнитель
**Исполнитель (Executor) – распределённый процесс, который отвечает за выполнение задач**. У каждого приложения Spark собственный набор исполнителей. Они работают в течение жизненного цикла отдельного приложения Spark.

* Исполнители делают всю обработку данных задания Spark
* Сохраняют результаты в памяти, а на диске – только тогда, когда это специально указывается в программе-драйвере (Driver Program).
* Возвращает результаты драйверу после их завершения.
*	Каждый узел может иметь от 1 исполнителя на узел до 1 исполнителя на ядро.

![Avatar](https://sun9-north.userapi.com/sun9-77/s/v1/ig2/asPWACs2Y8IwN5_YrsjrI6elFYprzRXym-SiMemEN6PuAKClwCZX0XNQ32-QQd_Na-bYfIeD1yrSUVLvr_CW8nT_.jpg?size=974x423&quality=96&type=album)

---
### Список используемых источников:

1. [Hadoop vs Spark: habr.com](https://habr.com/ru/company/luxoft/blog/569330/)
1. [Hadoop vs Spark: What's the Difference?](https://www.ibm.com/cloud/blog/hadoop-vs-spark)
1. [Hadoop vs Spark: detailed comparison](https://bestprogrammer.ru/programmirovanie-i-razrabotka/spark-ili-hadoop-podrobnoe-sravnenie)
1. [What Is Apache Spark?: YouTube](https://www.youtube.com/watch?v=znBa13Earms&t=1873s)
1. [What is the Spark Distributed Environment architecture?](https://spark-school.ru/blogs/spark-parallel-architecture/)
1. [Spark framework: proglib](https://proglib.io/p/spark-overview)


                     Щепетова Татьяна, 19-ПМ-2